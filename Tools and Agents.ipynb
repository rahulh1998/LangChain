{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047e7a3d",
   "metadata": {},
   "source": [
    "# Tools and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850fa3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul\\anaconda3\\envs\\genai\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "import subprocess\n",
    "# subprocess.run([\"ollama\",'pull',\"gemma3:270m\"],check=True)\n",
    "subprocess.run([\"ollama\",'pull',\"embeddinggemma:300m\"],check=True)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a719b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=wiki_wrapper)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac59ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a352aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\rahul\\\\anaconda3\\\\envs\\\\genai\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [wiki, arxiv]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a572f883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000022A67F551C0>, search_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Custom Tool\n",
    "docs = WebBaseLoader(\"https://docs.langchain.com/langsmith\").load()\n",
    "splits = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 100).split_documents(documents=docs)\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma:300m\")\n",
    "faiss = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "retriever = faiss.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fb01b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever=retriever, name='langsmith-search',description='Get information about langsmith')\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6872ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initilaise the model\n",
    "\n",
    "# llm = OllamaLLM(model='gemma3:270m')\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# chain = llm | parser\n",
    "# chain.invoke(\"Explain in simple terms why do we use feedforward networks in transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4971dc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Use the provided tools to answer the user’s questions.'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "# prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# prompt.messages\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# This mirrors the \"openai-functions-agent\" style prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided tools to answer the user’s questions.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # keeps track of history\n",
    "    (\"user\", \"{input}\"),                                # new user input\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),  # for reasoning/tool use\n",
    "])\n",
    "\n",
    "prompt.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32288ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000022A4FDD79C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000022A4FDD79C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Use the provided tools to answer the user’s questions.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000022A7C3A6120>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000022A7C3A72F0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'), streaming=True), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "llm = ChatGroq(model='gemma2-9b-it', streaming=True)\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdffa5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain  provides tools and components that simplify the process of using LLMs in various contexts, such as chatbot development, question answering, text summarization, and more.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about langsmith',\n",
       " 'chat_history': [],\n",
       " 'output': 'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain  provides tools and components that simplify the process of using LLMs in various contexts, such as chatbot development, question answering, text summarization, and more.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent,tools= tools, verbose=True)\n",
    "agent_executor.invoke({\"input\":\"tell me about langsmith\",\n",
    "                       \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b320b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The do\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The do\u001b[0m\u001b[32;1m\u001b[1;3mThis paper introduces the Transformer, a novel neural network architecture based solely on attention mechanisms.  It eliminates the need for recurrent or convolutional layers, which are traditionally used in sequence modeling tasks like machine translation. The Transformer's architecture allows for parallel computation, making it more efficient and scalable than previous models. \n",
      "\n",
      "\n",
      "The paper demonstrates the Transformer's effectiveness on a variety of machine translation tasks, achieving state-of-the-art results. It also provides insights into the attention mechanism and its role in capturing long-range dependencies in sequences. \n",
      "\n",
      "\n",
      "The Transformer has since become a foundational building block for many natural language processing (NLP) models, including BERT, GPT, and T5.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'explain this arxiv paper 1706.03762',\n",
       " 'chat_history': [],\n",
       " 'output': \"This paper introduces the Transformer, a novel neural network architecture based solely on attention mechanisms.  It eliminates the need for recurrent or convolutional layers, which are traditionally used in sequence modeling tasks like machine translation. The Transformer's architecture allows for parallel computation, making it more efficient and scalable than previous models. \\n\\n\\nThe paper demonstrates the Transformer's effectiveness on a variety of machine translation tasks, achieving state-of-the-art results. It also provides insights into the attention mechanism and its role in capturing long-range dependencies in sequences. \\n\\n\\nThe Transformer has since become a foundational building block for many natural language processing (NLP) models, including BERT, GPT, and T5.\\n\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"explain this arxiv paper 1706.03762\",\n",
    "                       \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81772879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'my first question'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Question\n",
      "Summary: A question is an utterance which serves as a request for information. Questions are sometimes distinguished from interrogatives, which are the grammatical forms, typically used\u001b[0m\u001b[32;1m\u001b[1;3mPlease provide me with the full text returned by the tool. \n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what was my first question about?',\n",
       " 'chat_history': [],\n",
       " 'output': 'Please provide me with the full text returned by the tool. \\n\\n\\n'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"what was my first question about?\",\n",
    "                       \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7332f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Attention Is All You Need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new de\u001b[0m\u001b[32;1m\u001b[1;3mThe paper introduces a new deep learning architecture called the Transformer, which relies entirely on attention mechanisms.  This means it doesn't use recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which were common in previous language models. \n",
      "\n",
      "Here are some key concepts from the paper:\n",
      "\n",
      "* **Attention:** This is the core mechanism of the Transformer. It allows the model to focus on different parts of the input sequence when processing information. This is like how humans pay attention to certain words in a sentence to understand its meaning.\n",
      "\n",
      "* **Encoder-Decoder:** The Transformer has two main parts: an encoder and a decoder. The encoder processes the input sequence and the decoder generates the output sequence. \n",
      "\n",
      "* **Multi-Head Attention:** \n",
      "The Transformer uses multiple attention heads in parallel. Each head attends to different aspects of the input, allowing the model to capture more complex relationships.\n",
      "\n",
      "* **Positional Encodings:** Since the Transformer doesn't have the sequential processing of RNNs, it needs a way to understand the order of words in a sequence. Positional encodings are added to the input embeddings to provide information about the position of each word.\n",
      "\n",
      "* **Feed-Forward Networks:**  After the attention layers, there are fully connected feed-forward networks that process the information further.\n",
      "\n",
      "The Transformer architecture has had a major impact on the field of natural language processing. It has been used in many successful applications, such as machine translation, text summarization, and question answering. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'in attention is all you need paper explain me all the major concepts',\n",
       " 'chat_history': [],\n",
       " 'output': \"The paper introduces a new deep learning architecture called the Transformer, which relies entirely on attention mechanisms.  This means it doesn't use recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which were common in previous language models. \\n\\nHere are some key concepts from the paper:\\n\\n* **Attention:** This is the core mechanism of the Transformer. It allows the model to focus on different parts of the input sequence when processing information. This is like how humans pay attention to certain words in a sentence to understand its meaning.\\n\\n* **Encoder-Decoder:** The Transformer has two main parts: an encoder and a decoder. The encoder processes the input sequence and the decoder generates the output sequence. \\n\\n* **Multi-Head Attention:** \\nThe Transformer uses multiple attention heads in parallel. Each head attends to different aspects of the input, allowing the model to capture more complex relationships.\\n\\n* **Positional Encodings:** Since the Transformer doesn't have the sequential processing of RNNs, it needs a way to understand the order of words in a sequence. Positional encodings are added to the input embeddings to provide information about the position of each word.\\n\\n* **Feed-Forward Networks:**  After the attention layers, there are fully connected feed-forward networks that process the information further.\\n\\nThe Transformer architecture has had a major impact on the field of natural language processing. It has been used in many successful applications, such as machine translation, text summarization, and question answering. \\n\\n\\n\\n\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"in attention is all you need paper explain me all the major concepts\",\"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41f6f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '2005.11401'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2021-04-12\n",
      "Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "Authors: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, He\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'abstract': True, 'query': '2005.11401'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2021-04-12\n",
      "Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "Authors: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, He\u001b[0m\u001b[32;1m\u001b[1;3mThis paper, \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",  published in 2021, explores a novel approach to improve the performance of language models on tasks requiring extensive knowledge. \n",
      "\n",
      "The core idea behind retrieval-augmented generation (RAG) is to combine the strengths of both text generation models, like GPT-3, and information retrieval systems. \n",
      "\n",
      "Traditionally, language models are trained on massive text datasets and learn to predict the next word in a sequence. While powerful, they often struggle with tasks demanding specific factual knowledge or reasoning. \n",
      "\n",
      "RAG addresses this by incorporating external knowledge sources.  When presented with a query, the RAG system first retrieves relevant information from a knowledge base or a large corpus of text. This retrieved information is then used to augment the input context for the language model, providing it with the necessary background knowledge to generate more accurate and relevant responses. \n",
      "\n",
      "The paper delves into the design and evaluation of different RAG architectures and techniques, showcasing how this approach significantly enhances the performance of language models on tasks such as question answering, summarization, and dialogue generation. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific aspect of the paper or RAG in general.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'explain 2005.11401 in 1000 words',\n",
       " 'chat_history': [],\n",
       " 'output': 'This paper, \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",  published in 2021, explores a novel approach to improve the performance of language models on tasks requiring extensive knowledge. \\n\\nThe core idea behind retrieval-augmented generation (RAG) is to combine the strengths of both text generation models, like GPT-3, and information retrieval systems. \\n\\nTraditionally, language models are trained on massive text datasets and learn to predict the next word in a sequence. While powerful, they often struggle with tasks demanding specific factual knowledge or reasoning. \\n\\nRAG addresses this by incorporating external knowledge sources.  When presented with a query, the RAG system first retrieves relevant information from a knowledge base or a large corpus of text. This retrieved information is then used to augment the input context for the language model, providing it with the necessary background knowledge to generate more accurate and relevant responses. \\n\\nThe paper delves into the design and evaluation of different RAG architectures and techniques, showcasing how this approach significantly enhances the performance of language models on tasks such as question answering, summarization, and dialogue generation. \\n\\n\\nLet me know if you\\'d like me to elaborate on any specific aspect of the paper or RAG in general.\\n'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"explain 2005.11401 in 1000 words\",\"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15440f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
