{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7725a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "os.environ['LANGCHAIN_PROJECT']\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY') # Not required to call this code explicitly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a736efe",
   "metadata": {},
   "source": [
    "## ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ddec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FCE4363E60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FCFF31B710>, root_client=<openai.OpenAI object at 0x000001FCFF31AF60>, root_async_client=<openai.AsyncOpenAI object at 0x000001FCFF31A870>, model_name='gpt-5-nano-2025-08-07', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the model\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-5-nano-2025-08-07')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdabcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Chelsea FC (Chelsea Football Club) is a professional soccer club based in London, England. Here are the basics:\\n\\n- Based in: Fulham, London\\n- Stadium: Stamford Bridge\\n- Founded: 1905\\n- Competition: Premier League (the top level of English football)\\n- Colors/Nickname: Royal blue; nicknamed “The Blues” (and historically “The Pensioners”)\\n- Notable achievements: one of England’s most successful clubs in the 21st century, with multiple domestic trophies (Premier League titles, FA Cups, League Cups) and major European honours (two UEFA Champions League titles and two UEFA Europa League titles)\\n- Ownership: owned by a consortium led by Todd Boehly and Clearlake Capital (as of the latest major changes)\\n\\nIf you’d like, I can give a brief history, talk about famous players, or cover their most memorable moments.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2175, 'prompt_tokens': 11, 'total_tokens': 2186, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1984, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CCzHUjdDTylldXx0ePqUToRetlQr2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--80bcd5b1-5154-4575-a7e4-15e207c0f764-0', usage_metadata={'input_tokens': 11, 'output_tokens': 2175, 'total_tokens': 2186, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1984}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke/Make a call from the model\n",
    "res1 = llm.invoke(\"What is Chelsea FC?\")\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7d6b1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chelsea FC (Chelsea Football Club) is a professional soccer club based in '\n",
      " 'London, England. Here are the basics:\\n'\n",
      " '\\n'\n",
      " '- Based in: Fulham, London\\n'\n",
      " '- Stadium: Stamford Bridge\\n'\n",
      " '- Founded: 1905\\n'\n",
      " '- Competition: Premier League (the top level of English football)\\n'\n",
      " '- Colors/Nickname: Royal blue; nicknamed “The Blues” (and historically “The '\n",
      " 'Pensioners”)\\n'\n",
      " '- Notable achievements: one of England’s most successful clubs in the 21st '\n",
      " 'century, with multiple domestic trophies (Premier League titles, FA Cups, '\n",
      " 'League Cups) and major European honours (two UEFA Champions League titles '\n",
      " 'and two UEFA Europa League titles)\\n'\n",
      " '- Ownership: owned by a consortium led by Todd Boehly and Clearlake Capital '\n",
      " '(as of the latest major changes)\\n'\n",
      " '\\n'\n",
      " 'If you’d like, I can give a brief history, talk about famous players, or '\n",
      " 'cover their most memorable moments.')\n"
     ]
    }
   ],
   "source": [
    "# Get the proper output format\n",
    "pprint(res1.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c5ed0",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07995581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Answer the questions in crisp and easy to understand manner'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a prompt Template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Answer the questions in crisp and easy to understand manner\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c403cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here’s a concise way to think about them and how they fit together.\\n'\n",
      " '\\n'\n",
      " 'What is a retriever?\\n'\n",
      " '- A component that, given a user query, fetches relevant documents or '\n",
      " 'passages from a large collection.\\n'\n",
      " '- It usually works by comparing the query to document representations '\n",
      " '(embeddings) or using keyword matches.\\n'\n",
      " '- Types:\\n'\n",
      " '  - Dense retrievers: use neural embeddings (high-dimensional vectors) to '\n",
      " 'measure similarity.\\n'\n",
      " '  - Sparse retrievers: rely on keyword-based methods like BM25.\\n'\n",
      " '  - Hybrid retrievers: combine dense and sparse signals.\\n'\n",
      " '\\n'\n",
      " 'What is a vector database?\\n'\n",
      " '- A specialized database that stores vector representations (embeddings) of '\n",
      " 'many items (documents, passages, etc.).\\n'\n",
      " '- It lets you perform fast similarity searches (nearest-neighbor search) to '\n",
      " 'find items whose vectors are closest to a query vector.\\n'\n",
      " '- Features often include: metadata filtering, hybrid search (combine vector '\n",
      " 'similarity with keyword filtering), scalability, and persistence.\\n'\n",
      " '\\n'\n",
      " 'How they work together (typical workflow):\\n'\n",
      " '1) Prepare data: convert documents into embeddings using a model (e.g., '\n",
      " 'sentence embeddings).\\n'\n",
      " '2) Store: save these embeddings in a vector database along with metadata '\n",
      " '(source, document ID, etc.).\\n'\n",
      " '3) Query: embed the user query.\\n'\n",
      " '4) Retrieve: use the vector DB to find the top-k documents with vectors '\n",
      " 'closest to the query vector.\\n'\n",
      " '5) Use: pass those retrieved documents to a reader/generator (like an LLM) '\n",
      " 'to produce an answer, often with re-ranking for quality.\\n'\n",
      " '\\n'\n",
      " 'Common examples:\\n'\n",
      " '- Vector databases: Pinecone, Weaviate, Milvus, Vespa, FAISS (library with '\n",
      " 'indexing that can back stores).\\n'\n",
      " '- Retrievers: components in frameworks like LangChain; can be dense '\n",
      " '(SBERT/ColBERT-style) or sparse (BM25) or hybrid.\\n'\n",
      " '\\n'\n",
      " 'Why they’re useful:\\n'\n",
      " '- They enable scalable, semantically aware search and QA over large text '\n",
      " 'collections.\\n'\n",
      " '- They’re essential for retrieval-augmented generation (RAG) and '\n",
      " 'knowledge-based chatbots.\\n'\n",
      " '\\n'\n",
      " 'Quick tips:\\n'\n",
      " '- Choose a retriever type based on needs: dense for semantic similarity, '\n",
      " 'sparse for exact keyword matches, hybrid for best of both.\\n'\n",
      " '- Ensure you have good quality embeddings and up-to-date data, since the '\n",
      " 'retriever’s effectiveness hinges on representation quality.\\n'\n",
      " '- Consider latency and cost: some vector DBs are optimized for real-time use '\n",
      " 'at scale; others are better for batch workflows.')\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "res2 = chain.invoke({\"input\":\"What are retrievers and vector databases?\"})\n",
    "\n",
    "pprint(res2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e96025",
   "metadata": {},
   "source": [
    "## OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73db4bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('- A tensor is a multi-dimensional array of numbers. It’s the generalization '\n",
      " 'of scalars, vectors, and matrices.\\n'\n",
      " '- Rank (or order):\\n'\n",
      " '  - 0D: scalar (e.g., 3.14)\\n'\n",
      " '  - 1D: vector (e.g., [1, 2, 3])\\n'\n",
      " '  - 2D: matrix (e.g., 3x4)\\n'\n",
      " '  - 3D+: higher-dimensional arrays (e.g., a stack of images, or a batch of '\n",
      " 'data)\\n'\n",
      " '- Shape: describes the size along each dimension. Examples:\\n'\n",
      " '  - scalar: ()\\n'\n",
      " '  - vector of length 5: (5,)\\n'\n",
      " '  - matrix 2x3: (2, 3)\\n'\n",
      " '  - image 64x64 RGB: (64, 64, 3)\\n'\n",
      " '  - batch of 32 images: (32, 64, 64, 3)\\n'\n",
      " '- Key properties: shape, data type (dtype), and device (CPU/GPU).\\n'\n",
      " '- Why it matters in ML: tensors are the primary data containers; they store '\n",
      " 'inputs, parameters, and outputs, support a wide range of operations '\n",
      " '(element-wise math, matmul, reductions), and can be tracked for automatic '\n",
      " 'differentiation.\\n'\n",
      " '\\n'\n",
      " 'In short: a tensor is a flexible, multi-dimensional data container used to '\n",
      " 'represent and compute with data in machine learning and scientific '\n",
      " 'computing.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "outputParser = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm|outputParser\n",
    "\n",
    "pprint(chain.invoke({\"What are tensors?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca410614",
   "metadata": {},
   "source": [
    "## Simple App#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e216ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LangSmith docs have moved to the LangChain Docs site. You can find the LangSmith documentation there.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://docs.smith.langchain.com/tutorials/Administrators/manage_spend\"\n",
    "\n",
    "wb_loader = WebBaseLoader(url)\n",
    "scraped_data = wb_loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "rct_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "splitted_data = rct_splitter.split_documents(scraped_data)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(splitted_data,embeddings)\n",
    "\n",
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]\n",
    "})\n",
    "\n",
    "retriever=vectorstoredb.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "\n",
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
